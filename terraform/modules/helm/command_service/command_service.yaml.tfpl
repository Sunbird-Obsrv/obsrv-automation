http_port: 8000
system_env: ${env}
image:
  repository: ${command_service_image_repository}
  tag: ${command_service_image_tag}
  imagePullSecrets: ${docker_registry_secret_name}

enable_lakehouse: ${enable_lakehouse}

hudi_jobs:
    - name: "LakehouseConnectorJob"
      release_name: lakehouse-connector
      job_manager_url: "lakehouse-connector-jobmanager.flink.svc.cluster.local:8081"


service_config: |


  commands:
    PUBLISH_DATASET:
      workflow:
        - MAKE_DATASET_LIVE
        - SUBMIT_INGESTION_TASKS
        - START_PIPELINE_JOBS
        - DEPLOY_CONNECTORS
        - CREATE_ALERT_METRIC
        - CREATE_AUDIT_EVENT
    RESTART_PIPELINE:
      workflow:
        - START_PIPELINE_JOBS

  alert_manager:
    metrics:
      - flink:
        - metric: "flink_taskmanager_job_task_operator_ExtractorJob_dataset_id_failed_event_count"
          alias: "Number of Failed Extraction Events"
          description: "This alert tracks how many events failed the extraction stage"
          frequency: ${lookup(datasetFailedBatchEventsConfig, "frequency")}
          interval: ${lookup(datasetFailedBatchEventsConfig, "interval")}
          operator: "gt"
          threshold: ${lookup(datasetFailedBatchEventsConfig, "threshold")}
        - metric: "flink_taskmanager_job_task_operator_ExtractorJob_dataset_id_duplicate_extraction_count"
          alias: "Number of Duplicate Extraction Events"
          description: "This alert tracks how many duplicate events were found during extraction stage"
          frequency: ${lookup(datasetDuplicateBatchEventsConfig, "frequency")}
          interval: ${lookup(datasetDuplicateBatchEventsConfig, "interval")}
          operator: "gt"
          threshold: ${lookup(datasetDuplicateBatchEventsConfig, "threshold")}
        - metric: "flink_taskmanager_job_task_operator_PipelinePreprocessorJob_dataset_id_duplicate_event_count"
          alias: "Number of Duplicate Preprocessing Events"
          description: "This alert tracks how many duplicate events were found during preprocessing stage"
          frequency: ${lookup(datasetDuplicateEventsConfig, "frequency")}
          interval: ${lookup(datasetDuplicateEventsConfig, "interval")}
          operator: "gt"
          threshold: ${lookup(datasetDuplicateEventsConfig, "threshold")}
        - metric: "flink_taskmanager_job_task_operator_PipelinePreprocessorJob_dataset_id_validation_failed_event_count"
          alias: "Number of Failed Validation Events"
          description: "This alert tracks how many events failed the validation stage"
          frequency: ${lookup(datasetValidationFailedEventsConfig, "frequency")}
          interval: ${lookup(datasetValidationFailedEventsConfig, "interval")}
          operator: "gt"
          threshold: ${lookup(datasetValidationFailedEventsConfig, "threshold")}
    object_connector_metrics:
      - metric: "sum_over_time(ObjectDiscoveryJob_cloud_authentication_failure{datasetId='dataset_id'}[1h])"
        alias: "Cloud Authentication Failure"
        description: "This alert tracks whether the cloud authentication failed for the dataset"
        frequency: 1h
        interval: 1h
        operator: "gt"
        threshold: 1
      - metric: "sum_over_time(ObjectProcessorJob_object_tag_update_failure{datasetId='dataset_id'}[1h])"
        alias: "Update Tag Failure"
        description: "This alert tracks whether the tag update failed for the dataset"
        frequency: 1h
        interval: 1h
        operator: "gt"
        threshold: 1
      - metric: "sum_over_time(ObjectDiscoveryJob_num_new_objects{datasetId='dataset_id'}[1d])"
        alias: "Number Of New Objects"
        description: "This alert tracks whether any new objects are processed. If not, then this alert is fired"
        frequency: 1d
        interval: 1h
        operator: "eq"
        threshold: 0
    jdbc_connector_metrics:
      - metric: "sum_over_time(JDBCConnectorJob_failure_count{datasetId='dataset_id'}[1d])"
        alias: "Number of Failed Records"
        description: "This alert tracks whether any records failed to be ingested into the database"
        frequency: 1d
        interval: 1h
        operator: "gt"
        threshold: 10
    masterdata_metrics:
    - metric: "MasterDataProcessorIndexerJob_failure_dataset_count{datasetId='dataset_id'}"
      alias: "Master Dataset failed to index"
      description: "This alert tracks whether the given master dataset failed to index data"
      frequency: 240m
      interval: 5m
      operator: "gt"
      threshold: 0

  postgres:
    db_host: postgresql-hl.postgresql.svc.cluster.local
    db_port: 5432
    db_user: ${postgresql_obsrv_username}
    db_password: ${postgresql_obsrv_user_password}
    database: ${postgresql_obsrv_database}

  config_service:
    host: dataset-api-service.dataset-api.svc.cluster.local
    port: 3000

  druid:
    router_host: "${druid_cluster_release_name}-routers.${druid_cluster_namespace}.svc.cluster.local"
    router_port: 8888
    supervisor_endpoint: indexer/v1/supervisor

  helm_charts_base_dir: /app/helm-charts

  spark:
    host: "http://spark-master-svc.spark.svc.cluster.local:8998/batches/"
    master:
      host: "spark://spark-master-svc.spark.svc.cluster.local:7077"
    driver:
      extraJavaOptions: "-Dlog4j.configuration=file:/opt/bitnami/spark/conf/log4j.properties"
    executor:
      extraJavaOptions: "-Dlog4j.configuration=file:/opt/bitnami/spark/conf/log4j.properties"

  connector_job:
    jdbc:
      - release_name: jdbc-connector
        jar: https://obsrv-connectors.s3.us-east-2.amazonaws.com/jdbc/jdbc-connector-1.0-jar-with-dependencies.jar
        class: org.sunbird.obsrv.job.JDBCConnectorJob
        schedule: "0 * * * *" # Every hour
        args:
          - "/opt/bitnami/spark/conf/jdbc-connector.conf"
    object:
      - release_name: object-discovery
        jar: https://obsrv-connectors.s3.us-east-2.amazonaws.com/object-store/object-discovery-1.0.0.jar
        class: in.sanketika.obsrv.job.ObjectDiscoveryJob
        schedule: "0 1 * * *" # Daily Midnight 1AM
        args:
          - /opt/bitnami/spark/conf/object-discovery.conf
      - release_name: object-processor
        jar: https://obsrv-connectors.s3.us-east-2.amazonaws.com/object-store/object-processor-1.0.0.jar
        class: in.sanketika.obsrv.job.ObjectProcessor
        schedule: "30 * * * *" # Every hour at 30 minutes
        args:
          - /opt/bitnami/spark/conf/object-processor.conf
  masterdata_job:
  - release_name: masterdata-indexer
    jar: https://obsrv-connectors.s3.us-east-2.amazonaws.com/masterdata-indexer/data-products-1.0.0.jar
    class: org.sunbird.obsrv.dataproducts.MasterDataProcessorIndexer
    schedule: "0 0 * * *" # Daily Midnight 12AM
    args:
      - /opt/bitnami/spark/conf/masterdata-indexer.conf

  kafka:
    brokers: ${kafka_release_name}-headless.${kafka_namespace}.svc.cluster.local:9092
    telemetry:
      topic: system.telemetry.events

  flink:
    namespace: ${flink_namespace}
    reinstall_sleep_time: 3
    jobs:
      - name: "UnifiedPipelineJob"
        release_name: unified-pipeline
        job_manager_url: "unified-pipeline-jobmanager.flink.svc.cluster.local:8081"
      - name: "MasterDataProcessorJob"
        release_name: master-data-processor-ext
        job_manager_url: "master-data-processor-ext-jobmanager.flink.svc.cluster.local:8081"
      - name: "KafkaConnectorJob"
        release_name: kafka-connector
        job_manager_url: "kafka-connector-jobmanager.flink.svc.cluster.local:8081"
      
      