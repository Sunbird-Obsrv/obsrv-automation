global:
  ssl_enabled: &ssl_enabled true
  cloud_storage_provider: &cloud_storage_provider "aws"
  cloud_store_provider: &cloud_store_provider "s3"
  cloud_storage_region: &cloud_storage_region "<fill-value>" # Update the region to deploy obsrv instance

  dataset_api_container: "<fill-value>"  # Update the dataset APIs bucket name
  config_api_container: "<fill-value>"   # Update the config APIs bucket name
  postgresql_backup_cloud_bucket: &backups_bucket "<fill-value>" # Update with the backup bucket name
  redis_backup_cloud_bucket: &redis_backup_cloud_bucket "<fill-value>" # Update with the backup bucket name
  velero_backup_cloud_bucket: &velero_backup_cloud_bucket "<fill-value>" # Update with velero backup  bucket name
  cloud_storage_bucket: &cloud_storage_bucket "<fill-value>" # Update with the name of cloud storage bucket
  hudi_metadata_bucket: &hudi_metadata_bucket "s3a://<fill-value>/hudi" # Update with hudi bucket name

  # Update the configuration by replacing 'name' with the access key, 'key' with the secret key, and 'region-name' with the specific region for access.
  cloud_storage_config: |+
    '{"identity":"name","credential":"key","region":"region-name"}'

  spark_cloud_bucket: ""
  storage_class_name: &storage_class_name "gp2"
  spark_service_account_arn: ""
  secor_backup_bucket: &secor_backup_bucket "<fill-value>" # Update with secor bucket name
  checkpoint_bucket: &checkpoint_bucket "s3://<fill-value>" # Update with checkpoint bucket name
  deep_store_type: &deep_store_type "s3"
  s3_access_key: &s3-access-key "<fill-value>" # Update with the AWS access key
  s3_secret_key: &s3-secret-access-key "<fill-value>" # Update with the AWS secret key
  s3_endpoint_url: &s3-endpoint-url ""
  kong_annotations: &kong_annotations
    service.beta.kubernetes.io/aws-load-balancer-type: nlb
    service.beta.kubernetes.io/aws-load-balancer-scheme: internet-facing
    service.beta.kubernetes.io/aws-load-balancer-eip-allocations: "<fill-value>" # Update with the Elastic IP allocation
    service.beta.kubernetes.io/aws-load-balancer-subnets: "<fill-value>" # Update with the public subnet ID

service_accounts:
  enabled: &create_sa true
  secor: &secor_sa_annotation
    eks.amazonaws.com/role-arn: "<fill-value>"  # Update with the ARN of the IAM role for this service account
  dataset_api: &dataset_api_sa_annotation
    eks.amazonaws.com/role-arn: "<fill-value>" # Update with the ARN of the IAM role for this service account
  config_api: &config_api_sa_annotation
    eks.amazonaws.com/role-arn: "<fill-value>" # Update with the ARN of the IAM role for this service account
  druid_raw: &druid_raw_sa_annotation
    eks.amazonaws.com/role-arn: "<fill-value>" # Update with the ARN of the IAM role for this service account
  flink: &flink_sa_annotation
    eks.amazonaws.com/role-arn: "<fill-value>" # Update with the ARN of the IAM role for this service account
  postgresql_backup: &postgresql_backup_sa_annotation
    eks.amazonaws.com/role-arn: "<fill-value>" # Update with the ARN of the IAM role for this service account
  redis_backup: &redis_backup_sa_annotation
    eks.amazonaws.com/role-arn: "<fill-value>" # Update with the ARN of the IAM role for this service account
  s3_exporter: &s3_exporter_sa_annotation
    eks.amazonaws.com/role-arn: "<fill-value>" # Update with the ARN of the IAM role for this service account
  spark: &spark_sa_annotation
    eks.amazonaws.com/role-arn: "<fill-value>" # Update with the ARN of the IAM role for this service account


redis-service-account: &redis-service-account
  serviceAccount:
    create: *create_sa
    name: redis-backup-sa
    annotations:
      <<: *redis_backup_sa_annotation

redis_dedup_backup_sidecar: &redis_dedup_backup_sidecar
  master:
    sidecars:
      - name: redis-backup
        image: sanketikahub/redis-backup:1.0.6-GA
        volumeMounts:
          - mountPath: "/data"
            name: redis-data
        env:
          - name: REDIS_BACKUP_CRON_SCHEDULE
            value: "00 00 * * *"
          - name: CLOUD_SERVICE
            value: *cloud_store_provider
          - name: S3_BACKUP_BUCKET
            value: *redis_backup_cloud_bucket
          - name: GCS_BACKUP_BUCKET
            value: *redis_backup_cloud_bucket
          - name: REDIS_RDB_FILE_PATH
            value: "/data"
          - name: REDIS_REPLICATION_MODE
            value: master
          - name: AZURE_STORAGE_ACCOUNT_NAME
            value: "name"
          - name: AZURE_STORAGE_ACCOUNT_KEY
            value: "key"
          - name: AZURE_BACKUP_BUCKET
            value: "backup"
          - name: BACKUP_PREFIX
            value: dedup-redis
        resources:
          limits:
            cpu: 0.2
            memory: 100Mi
    <<: *redis-service-account

redis_denorm_backup_sidecar: &redis_denorm_backup_sidecar
  master:
    sidecars:
      - name: redis-backup
        image: sanketikahub/redis-backup:1.0.6-GA
        volumeMounts:
          - mountPath: "/data"
            name: redis-data
        env:
          - name: REDIS_BACKUP_CRON_SCHEDULE
            value: "00 00 * * *"
          - name: CLOUD_SERVICE
            value: *cloud_store_provider
          - name: S3_BACKUP_BUCKET
            value: *redis_backup_cloud_bucket
          - name: GCS_BACKUP_BUCKET
            value: *redis_backup_cloud_bucket
          - name: REDIS_RDB_FILE_PATH
            value: "/data"
          - name: REDIS_REPLICATION_MODE
            value: master
          - name: AZURE_STORAGE_ACCOUNT_NAME
            value: "name"
          - name: AZURE_STORAGE_ACCOUNT_KEY
            value: "key"
          - name: AZURE_BACKUP_BUCKET
            value: "backup"
          - name: BACKUP_PREFIX
            value: denorm-redis
        resources:
          limits:
            cpu: 0.2
            memory: 100Mi
    <<: *redis-service-account

s3-exporter-service-account: &s3-exporter-service-account
  s3_region: *cloud_storage_region
  serviceAccount:
    create: *create_sa
    name: s3-exporter-sa
    annotations:
      <<: *s3_exporter_sa_annotation

velero-backup: &velero-backup
  credentials:
    useSecret: true
    secretContents:
      cloud: |
        [default]
        aws_access_key_id="<fill-value>"   # Update with the AWS access key
        aws_secret_access_key="<fill-value>" # Update with the AWS secret key
  configuration:
    provider: *cloud_storage_provider
    backupStorageLocation:
      bucket: *velero_backup_cloud_bucket
      config:
        region: *cloud_storage_region
    volumeSnapshotLocation:
      name: default
      config:
        region: *cloud_storage_region
  initContainers:
    - name: velero-plugin-for-aws
      image: velero/velero-plugin-for-aws
      volumeMounts:
        - name: plugins
          mountPath: /target

spark:
  persistence:
    masterTmp:
      storageClassName: *storage_class_name
    workerTmp:
      storageClassName: *storage_class_name
    masterMetadata:
      storageClassName: *storage_class_name
    workerMetadata:
      storageClassName: *storage_class_name
  cloud_provider: *cloud_storage_provider

dataset-api-service-account: &dataset-api-service-account
  serviceAccount:
    create: *create_sa
    annotations: {}

redis-denorm:
  << : *redis_denorm_backup_sidecar

redis-dedup:
  << : *redis_dedup_backup_sidecar

velero:
  <<: *velero-backup

s3-exporter:
  <<: *s3-exporter-service-account

dataset-api:
  <<: *dataset-api-service-account

secor-service-account: &secor-service-account
  serviceAccount:
    create: *create_sa
    name: secor-sa
    annotations:
      <<: *secor_sa_annotation

secor:
  cloud_store_provider: *cloud_store_provider
  s3_bucket_name: *backups_bucket
  s3_region: *cloud_storage_region
  storageClass: *storage_class_name
  <<: *secor-service-account

kong:
  proxy:
    annotations:
      <<: *kong_annotations
    type: LoadBalancer

flink-service-account: &flink-service-account
  serviceAccount:
    create: *create_sa
    name: flink-sa
    annotations:
      <<: *flink_sa_annotation

flink:
  <<: *flink-service-account
  checkpoint_store_type: *cloud_store_provider
  checkpointing:
    enabled: true
    statebackend: *checkpoint_bucket

postgres-backup-service-account: &postgres-backup-service-account
  serviceAccount:
    create: *create_sa
    name: postgresql-backup-sa
    annotations:
      <<: *postgresql_backup_sa_annotation

postgresql-backup:
  <<: *postgres-backup-service-account

druid-raw-service-account: &druid-raw-service-account
  serviceAccount:
    create: *create_sa
    name: druid-raw-sa
    annotations:
      <<: *druid_raw_sa_annotation

druid-raw-cluster:
  druid_deepstorage_type: *deep_store_type
  druid_indexer_logs_type: *deep_store_type
  storageClass: *storage_class_name
  s3_bucket: *cloud_storage_bucket
  s3_access_key: ""
  s3_secret_key: ""
  <<: *druid-raw-service-account

loki:
  storage:
    type: *storage_class_name
  compactor:
    retention_enabled: true
    working_directory: /var/loki/compactor/retention

lakehouse-connector:
  hadoop_core_site:
      fs.s3a.imp: org.apache.hadoop.fs.s3a.S3AFileSystem
      fs.s3a.access.key: *s3-access-key
      fs.s3a.secret.key: *s3-secret-access-key
  serviceAccount:
    create: false
    name: flink-sa
  checkpointing:
    enabled: true
    statebackend: *hudi_metadata_bucket
    annotations:
      <<: *flink_sa_annotation

trino:
  additionalCatalogs:
    lakehouse: |-
      connector.name=hudi
      hive.metastore.uri=thrift://hudi-hms.hms.svc:9083
      hive.s3.aws-access-key=<fill-value>
      hive.s3.aws-secret-key=<fill-value>
      hive.s3.ssl.enabled=false

hms:
  envVars:
    WAREHOUSE_DIR: *hudi_metadata_bucket


  hadoop_core_site:
    fs.s3a.access.key:  *s3-access-key
    fs.s3a.secret.key:  *s3-secret-access-key
    # fs.s3a.endpoint: *s3-endpoint-url
    # fs.s3a.path.style.access: false
    # fs.s3a.connection.ssl.enabled: true

letsencrypt-ssl:
  enabled: *ssl_enabled