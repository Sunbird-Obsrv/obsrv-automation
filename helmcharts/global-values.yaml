defaults:
  env: &env "dev"
  building-block: &building-block "obsrv"
  encryption_key: &encryption-key "strong_encryption_key_to_encrypt"
  namespaces:
    kafka_namespace: &kafka-namespace "kafka"
    postgresql_namespace: &postgresql-namespace "postgresql"
    druid_namespace: &druid-namespace "druid-raw"
    redis_namespace: &redis-namespace "redis"
    flink_namespace: &flink_namespace "flink"
    loki_namespace: &loki-namespace "loki"
    monitoring_namespace: &monitoring-namespace "monitoring"
    spark_namespace: &spark-namespace "spark"
    superset_namespace: &superset-namespace "superset"
    secor_namespace: &secor-namespace "secor"
    command_api_namespace: &command-api-namespace "command-api"
    dataset_api_namespace: &dataset-api-namespace "dataset-api"
    config_api_namespace: &config-api-namespace "config-api"
    kubernetes_reflector_namespace: &kubernetes-reflector-namespace "kubernetes-reflector"
    submit_ingestion_namespace: &submit-ingestion-namespace "submit-ingestion"
    kong_namespace: &kong-namespace "kong"
    kong_ingress_namespace: &kong-ingress-namespace "kong-ingress"
    system_rules_ingestor: &system-rules-ingestor-namespace "system-rules-ingestor"
    web_console_namesapce: &web-console-namesapce "web-console"
    cert_manager_namespace: &cert-manager-namespace "cert-manager"
    velero_namespace: &velero-namespace "velero"
    volume_autoscaler_namespace: &volume-autoscaler-namespace "volume-autoscaler"
    hms_namespace: &hms_namespace "hms"
    trino_namespace: &trino_namespace "trino"
    nlq_api_namespace: &nlq_api_namespace "nlq"

  postgres:
    pghost: &pghost "postgresql-hl.postgresql.svc.cluster.local"
    obsrv_username: &psql-obsrv-user "obsrv"
    obsrv_database: &psql-obsrv-db "obsrv"
    obsrv_user_password: &psql-obsrv-pwd "obsrv123"
    druid_database: &psql-druid-db "druid_raw"
    druid_username: &psql-druid-user "druid_raw"
    druid_user_password: &psql-druid-pwd "druidraw123"
    superset_db_name: &psql-superset-db "superset"
    superset_user_name: &psql-superset-user "superset"
    superset_user_password: &psql-superset-pwd "superset123"
    hms_db_name: &psql-hms-db "hms"
    hms_user_name: &psql-hms-user "hms"
    hms_user_password: &psql-hms-pwd "hms123"

  grafana:
    grafana_admin_user: &grafana-admin-user "admin"
    grafana_admin_password: &grafana-admin-password "adminpassword"
    grafana_auth_token: &grafana-auth-token "YWRtaW46YWRtaW5wYXNzd29yZA=="
    grafana_url: &grafana-url "http://grafana.monitoring.svc.cluster.local"

  kafka-topics:
    ingestTopic: &ingestTopic "dev.ingest"
    system_stats_topic: &system_stats_topic "dev.stats"
    masterdata_system_stats_topic: &masterdata_system_stats_topic "dev.masterdata.stats"
    masterdataIngestTopic: &masterdataIngestTopic "dev.masterdata.ingest"
    telemetryEventsTopic: &telemetryEventsTopic "system.telemetry.events"
    hudiConnectorTopic: &hudiConnectorTopic "dev.hudi.connector.in"
    connectorsMetricTopic: &connectorsMetricTopic "obsrv-connectors-metrics"
    connectorsFailedTopic: &connectorsFailedTopic "connectors.failed"

  # config_service_host: &config-service-host "http://command-api.command-api.svc.cluster.local"
  # druid_host: &global-druid-host "http://druid-raw-routers.druid-raw.svc.cluster.local"
  druid_url: &global-druid-url "http://druid-raw-routers.druid-raw.svc:8888"
  dataset_service_url: &dataset-service-url "http://dataset-api.dataset-api.svc.cluster.local:3000"
  config_service_url: &config-service-url "http://config-api.config-api.svc.cluster.local:4000"

  cron_schedule: &cron-schedule "0 0 * * *"
  druid_router_host: &druid-router-host "druid-raw-routers.druid-raw.svc.cluster.local"
  druid_indexer_host: &druid-indexer-host "druid-raw-indexers.druid-raw.svc.cluster.local"

  redirection_auth_path: &redirection_auth_path ""
  domain: &domain "<fill-value>.sslip.io"  # Update with the Allocation ID of the Elastic IP 



  ssl_enabled: &ssl_enabled true

  domain_admin_email: &domain_admin_email "test@obsrv-ai.com"

#coredb-charts/
kafka: &kafka
  namespace: *kafka-namespace
  host: "kafka-headless.kafka.svc.cluster.local"
  port: 9092
  bootstrap-server: &kafka-bootstrap-server "kafka-headless.kafka.svc.cluster.local:9092"
  ingestTopic: *ingestTopic
  masterdataIngestTopic: *masterdataIngestTopic
  telemetryEventsTopic: *telemetryEventsTopic
  ss-kafka-topic: *system_stats_topic
  masterdata-ss-kafka-topic: *masterdata_system_stats_topic
  hudiConnectorTopic: *hudiConnectorTopic
  connectorsMetricTopic: *connectorsMetricTopic
  connectorsFailedTopic: *connectorsFailedTopic
  numPartitions: 4
  provisioning:
    numPartitions: 4
    replicationFactor: 1
    topics:
      - name: *ingestTopic
        partitions: 4
        replicationFactor: 1
      - name: *masterdataIngestTopic
        partitions: 4
        replicationFactor: 1
      - name: *telemetryEventsTopic
        partitions: 4
        replicationFactor: 1
      - name: *system_stats_topic
        partitions: 4
        replicationFactor: 1
      - name: *masterdata_system_stats_topic
        partitions: 4
        replicationFactor: 1
      - name: *hudiConnectorTopic
        partitions: 4
        replicationFactor: 1
      - name: *connectorsMetricTopic
        partitions: 4
        replicationFactor: 1
      - name: *connectorsFailedTopic
        partitions: 4
        replicationFactor: 1


zookeeper: &zookeeper
  namespace: *kafka-namespace
  host: "kafka-zookeeper-headless.kafka.svc.cluster.local"
  port: 2181

postgresql: &postgresql
  namespace: *postgresql-namespace
  # the below variables are not chart variables
  # these are defined so that it can used used in other charts
  # changing the below value has no effect on actual values
  # refer to the chart for actual variables
  # update below values manually if actual you change actual values
  host: *pghost
  port: 5432
  #################################################################
  # !!! If bitnami charts is used, **username should be postgres**.
  #################################################################
  username: &pguser "postgres"
  password: &pgpassword "postgres"

  obsrv:
    name: *psql-obsrv-db
    user: *psql-obsrv-user
    password: *psql-obsrv-pwd

  superset:
    name: *psql-superset-db
    user: *psql-superset-user
    password: *psql-superset-pwd

  druid:
    name: *psql-druid-db
    user: *psql-druid-user
    password: *psql-druid-pwd

  hms:
    name: *psql-hms-db
    user: *psql-hms-user
    password: *psql-hms-pwd

  # Internal. Don't touch
  auth:
    enablePostgresUser: true
    postgresPassword: *pgpassword

  primary:
    initdb:
      user: *pguser
      password: *pgpassword
      scripts:
        00_create_superset_db.sql: |
          CREATE DATABASE superset;
        01_create_druid_raw_db.sql: |
          CREATE DATABASE druid_raw;
        02_create_obsrv_db.sql: |
          CREATE DATABASE obsrv;
        03_create_pg_stat_statements_db.sql: |
          CREATE EXTENSION IF NOT EXISTS pg_stat_statements;
        04_create_hms_db.sql: |
          CREATE DATABASE hms;
    extendedConfiguration: |
      password_encryption = md5
      shared_preload_libraries = 'pg_stat_statements' # (change requires restart)
      pg_stat_statements.max = 10000
      pg_stat_statements.track = all
      log_statement = 'all'
      logging_collector = 'on'
      log_min_duration_statement = 0
      log_filename = 'postgresql.log'
      log_directory='/opt/bitnami/postgresql/logs'
    persistence:
      size: 10Gi
      enabled: true
      mountPath: /bitnami/postgresql

druid: &druid
  namespace: *druid-namespace
  host: "druid-raw-routers.druid-raw.svc.cluster.local"
  port: 8888
  supervisorEndpoint: "indexer/v1/supervisor"

redis-denorm: &redis_denorm
  namespace: *redis-namespace
  host: redis-denorm-headless.redis.svc.cluster.local
  port: 6379

redis-dedup: &redis_dedup
  namespaceOverride: *redis-namespace #TODO: check why namespace  doesn't work here
  host: &redis-url redis-dedup-headless.redis.svc.cluster.local
  port: 6379

flink: &flink
  namespace: *flink_namespace
  host: "flink-headless.flink.svc.cluster.local"
  port: 8081

#migrations/charts/
postgresql-migration:
  namespace: *postgresql-namespace
  superset_oauth_clientid: &superset_oauth_clientid "451058501-dev.oauth.obsrv.ai"
  superset_oauth_client_secret: &superset_oauth_client_secret "luXRJMh"
  kong_ingress_domain: *domain
  gf_auth_generic_oauth_client_id: "528806583-dev.oauth.obsrv.ai"
  gf_auth_generic_oauth_client_secret: "el642dcXd1P3v6i+hODnGrUKx9ZSWAlmXWZaEoZQI7/R3NUGQlLTnNCV"
  web_console_password: "$2a$10$bG9R7ioA4/pfw8m0GPcWTOZMhc2sNN4wEkKV.j50RvQW5iUki/4Za"
  web_console_login: "admin@obsrv.in"
  system_settings:
    encryption_key: "strong_encryption_key_to_encrypt"
    default_dataset_id: "ALL"
    max_event_size: 1048576
    dedup_period: 604800 # In seconds (7 days)

#coreinfra-charts/
prometheus: &prometheus
  url: http://monitoring-kube-prometheus-prometheus.monitoring.svc.cluster.local:9090

loki:
  namespace: *loki-namespace


druid-raw-cluster: &druid-raw-cluster
  namespace: *druid-namespace
  druid_metadata_storage_connector_user: druid_raw
  druid_metadata_storage_connector_password: *psql-druid-pwd
  zookeeper:
    namespace: *druid-namespace

druid-operator:
  namespace: *druid-namespace
  # nameOverride: druid-operator

kafka-message-exporter:
  namespace: *kafka-namespace
  exporterEnv:
    KAFKA_BROKERS: *kafka-bootstrap-server
    KAFKA_CONSUMER_GROUP_ID: obsrv-kafka-message-exporter

kube-prometheus-stack:
  namespaceOverride: *monitoring-namespace
  prometheus-windows-exporter:
   namespaceOverride: *monitoring-namespace
  kube-state-metrics:
    namespaceOverride: *monitoring-namespace
  prometheus-node-exporter:
    namespaceOverride: *monitoring-namespace
  grafana:
    fullnameOverride: "grafana"
    namespaceOverride: *monitoring-namespace 
    env:
      GF_SECURITY_ADMIN_PASSWORD: *grafana-admin-password
      GF_SECURITY_ADMIN_USER: *grafana-admin-user
    grafana.ini:
      smtp:
        enabled: true
        host: ""
        user: ""
        password: ""
        from_address: ""
        cert_file: ""
        key_file: ""
        ehlo_identity: ""
        startTLS_policy: ""
        skip_verify: true
        from_name: "Grafana"    
  

kubernetes-reflector:
  namespace: *kubernetes-reflector-namespace

postgresql-backup:
  namespace: *postgresql-namespace
  cronSchedule: *cron-schedule
  PG_USER: *pguser
  PG_HOST: *pghost
  PGPASSWORD: *pgpassword
  initContainers: {}

postgresql-exporter:
  config:
    datasource:
      host: *pghost
      user: *pguser
      password: *pgpassword


spark: &spark
  namespace: *spark-namespace

superset_oauth: &superset_oauth 
  enabled: true
  client_id: *superset_oauth_clientid
  client_secret: *superset_oauth_client_secret
  auth_token: "NDUxMDU4NTAxLWRldi5vYXV0aC5vYnNydi5haTpsdVhSSk1o"

superset:
  namespace: *superset-namespace
  postgres:
    adminUser: *pguser
    adminPassword: *pgpassword
    db_host: *pghost #postgresql-hl.postgresql.svc.cluster.local (Actual value) Need to be checked
    db_port: "5432"
    superset:
      db_name: *psql-superset-db
      db_username: *psql-superset-user
      db_password: *psql-superset-pwd
  oauth: 
    <<: *superset_oauth

command-api: &command-api
  namespace: *command-api-namespace
  system_env: *env

dataset-api: &dataset-api
  namespace: *dataset-api-namespace
  host: *dataset-service-url
  env:
    GRAFANA_AUTH_TOKEN: *grafana-auth-token
    GRAFANA_ADMIN_URL: *grafana-url

config-api: &config-api
  namespace: *config-api-namespace
  host: *config-service-url
  env:
    GRAFANA_AUTH_TOKEN: *grafana-auth-token
    GRAFANA_ADMIN_URL: *grafana-url


grafana-configs:
  namespace: *monitoring-namespace

submit-ingestion:
  namespace: *submit-ingestion-namespace

system-rules-ingestor:
  namespace: *system-rules-ingestor-namespace
  datasetServiceUrl: *dataset-service-url
  grafana:
    namespace: *monitoring-namespace
    containerName: grafana
    fileNames: "api.yaml,ingestion.yaml,processing.yaml,querying.yaml,node.yaml,storage.yaml,monitoring.yaml"

web-console:
  namespace: *web-console-namesapce
  env:
    GRAFANA_AUTH_TOKEN: *grafana-auth-token
    GRAFANA_ADMIN_URL: *grafana-url

#additional-charts/
alert-rules: &alert-rules
  namespace: *monitoring-namespace

druid-exporter:
  namespace: *druid-namespace
  druidURL: *global-druid-url
  serviceMonitor:
    enabled: true
    namespace: *druid-namespace
    interval: 30s
    scrapeTimeout: 10s

secor:
  namespace: *secor-namespace
  # kafka: *kafka
  # zookeeper: *zookeeper

cert-manager: &cert-manager
  namespace: *cert-manager-namespace

kong:
  namespace: *kong-ingress-namespace
  extraLabels:
    system.api: "true"
  podLabels:
    system.api: "true"
  proxy:
    annotations: {}
    labels:
      system.api: "true"
  serviceMonitor:
    enabled: true
    interval: 30s
    namespace: *kong-ingress-namespace
    labels:
      release: monitoring
      system.api: "true"

kong-ingress-routes:
  namespace: *kong-ingress-namespace
  domain: *domain

letsencrypt-ssl:
  enabled: *ssl_enabled
  namespace: *web-console-namesapce
  cert_issuer_name: "letsencrypt-prod"
  # prod letsencrypt url, to be used on prod instances
  letsencrypt_server_url: "https://acme-v02.api.letsencrypt.org/directory"
  # letsencrypt_server_url: "https://acme-staging-v02.api.letsencrypt.org/directory"
  domain: *domain
  domain_admin_email: *domain_admin_email

velero:
  namespace: *velero-namespace
  metrics:
    service:
      labels:
        release: monitoring
    enabled: true
    additionalLabels:
      release: monitoring
    serviceMonitor:
      enabled: true
      namespace: *velero-namespace
      additionalLabels:
        release: monitoring
      autodetect: true
  schedules:
    obsrv-daily-backup:
      disabled: false
      schedule: "0 0 * * *"
      useOwnerReferencesInBackup: false
      template:
        ttl: "247h"
        includedNamespaces: []

volume-autoscaler:
  namespace: *volume-autoscaler-namespace
  # How much full the disk must be before considering scaling
  scale_above_percent: "80"
  # How much to scale disks up by, as a percentage of their current size
  scale_up_percent: "20"
  # An minimum amount of bytes to scale up by (typically because providers like AWS only support 100GB increments in disk size)
  scale_up_min_increment: "107374182400"
  # An maximum amount of bytes to scale up by (helps prevent large disks from growing too fast/exponentially).  Set to 16TB by default, which basically means this is disabled
  scale_up_max_increment: ""
  # The maximum size of disks to scale up to.  By default on AWS using EBS volumes this is set to 16TB as that is the EBS Max disk size.
  scale_up_max_size: ""

hms:
  namespace: *hms_namespace
  envVars:
    DATABASE_HOST: *pghost
    DATABASE_DB: *psql-hms-db
    DATABASE_USER: *psql-hms-user
    DATABASE_PASSWORD: *psql-hms-pwd
    THRIFT_PORT: "9083"

trino:
  namespace: *trino_namespace

lakehouse-connector:
  namespace: *flink_namespace

nlq-api:
  namespace: *nlq_api_namespace
  openai_api_key: "<OPENAI_API_KEY>"

global:
  env: *env
  building_block: *building-block
  encryption_key: *encryption-key
  domain: *domain
  ssl_enabled: *ssl_enabled
  kafka: *kafka
  zookeeper: *zookeeper
  druid: *druid
  postgresql: *postgresql
  flink: *flink
  redis_denorm: *redis_denorm
  redis_dedup: *redis_dedup
  druid-raw-cluster: *druid-raw-cluster
  spark: *spark
  cert-manager: *cert-manager
  alert_rules: *alert-rules
  command-api: *command-api
  dataset_api: *dataset-api
  storageClass: ""
  # This redis is used by flink
  redis: *redis_denorm
  prometheus: *prometheus