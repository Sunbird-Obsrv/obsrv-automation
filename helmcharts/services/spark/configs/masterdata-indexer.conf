env="{{ .Values.global.env }}"

kafka {
    bootstrap.servers =  "{{ .Values.global.kafka.host }}:{{ .Values.global.kafka.port }}"
    topic.ingest      = "{{ .Values.global.kafka.ingestTopic}}"
}

postgres {
    host = "{{ .Values.global.postgresql.host }}"
    port = "{{ .Values.global.postgresql.port }}"
    user = "{{ .Values.postgresqlUser | default .Values.global.postgresql.obsrv.user }}"
    password = "{{ .Values.postgresqlPassword | default .Values.global.postgresql.obsrv.password }}"
    database = "{{ .Values.postgresqlDatabase | default .Values.global.postgresql.obsrv.name }}"
    maxConnections = 2
}

redis.host = "{{ .Values.global.redis_denorm.host }}"
redis.port = "{{ .Values.global.redis_denorm.port }}"
redis.scan.count = 1000
redis.max.pipeline.size = 1000
cloud.storage.container = "://obsrv-connectors/"
cloud.storage.provider = "aws"
cloud.storage.accountName = ""
druid.indexer.url = "http://{{ .Values.global.druid.host }}:{{ .Values.global.druid.port }}/druid/indexer/v1/task"
druid.datasource.delete.url = "http://{{ .Values.global.druid.host }}:{{ .Values.global.druid.port }}/druid/coordinator/v1/datasources"

metrics {
    topicName = "${{ .Values.global.env }}.spark.stats"
}

#inputSourceSpec
source.spec="{\"spec\":{\"ioConfig\":{\"type\":\"index_parallel\",\"inputSource\":{\"type\":\"${cloud.storage.provider}\",\"objectGlob\":\"**.json.gz\",\"prefixes\":[\"FILE_PATH\"]}}}}"
#deltaIngestionSpec
delta.ingestion.spec= "{\"type\":\"index_parallel\",\"spec\":{\"dataSchema\":{\"dataSource\":\"DATASOURCE_REF\"},\"ioConfig\":{\"type\":\"index_parallel\"},\"tuningConfig\":{\"type\":\"index_parallel\",\"maxRowsInMemory\":500000,\"forceExtendableShardSpecs\":false,\"logParseExceptions\":true}}}"