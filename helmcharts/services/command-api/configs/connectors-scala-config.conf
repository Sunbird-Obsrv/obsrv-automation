env = "{{ .Values.global.env }}"
building-block = "{{ .Values.global.building_block}}"
spark.master= "local[*]"

kafka {
  producer = {
    broker-servers =  "{{ .Values.global.kafka.host }}:{{ .Values.global.kafka.port }}"
    max-request-size = 1000000 # 1MB
    linger.ms = 10
    batch.size = 98304
    compression = "snappy"
  }
  output = {
    connector = {
      failed.topic = "{{ .Values.global.kafka.connectorsFailedTopic }}"
      metric.topic = "{{ .Values.global.kafka.connectorsMetricTopic }}"
    }
  }
}

#dataset-registry config
postgres {
  host = "{{ .Values.global.postgresql.host }}"
  port = "{{ .Values.global.postgresql.port }}"
  maxConnections = 2
  user = "{{ .Values.postgresqlUser | default .Values.global.postgresql.obsrv.user }}"
  password = "{{ .Values.postgresqlPassword | default .Values.global.postgresql.obsrv.password }}"
  database = "{{ .Values.postgresqlDatabase | default .Values.global.postgresql.obsrv.name }}"
}

obsrv.encryption.key = "{{ .Values.global.encryption_key }}"

task {
  checkpointing.compressed = true
  checkpointing.interval = 60000
  checkpointing.pause.between.seconds = 30000
  restart-strategy.attempts = 3
  restart-strategy.delay = 30000 # in milli-seconds
  parallelism = 1
  consumer.parallelism = 1
  downstream.operators.parallelism = 1
}
