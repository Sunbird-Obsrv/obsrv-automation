namespace: "flink"
imagepullsecrets: ""
image:
  registry: sanketikahub
  repository: lakehouse-connector
  tag: 1.0.8
serviceMonitor:
  enabled: false
replicaCount: 1

jobmanager:
  rpc_port: 6123
  blob_port: 6124
  query_port: 6125
  ui_port: 8081
  prom_port: 9250
  heap_memory: 1024
  cpu_requests: 1
  cpu_limits: 1
  memory_requests: 1024Mi
  memory_limits: 1024Mi

rest_port: 80
resttcp_port: 8081
service:
  type: NodePort

taskmanager:
  prom_port: 9251
  rpc_port: 6122
  heap_memory: 1024
  replicas: 1
  cpu_requests: 1
  cpu_limits: 1
  memory_requests: 1024Mi
  memory_limits: 2300Mi

checkpoint_store_type: "s3"

# AWS S3 Details
s3_access_key: ""
s3_secret_key: ""
s3_endpoint: ""

# Azure Container Details
azure_account: ""
azure_secret: ""

postgres:
  max_connections: 2
  sslmode: false

global:
  redis:
    host: redis-denorm.redis.svc.cluster.local
    port: 6379
  cassandra:
    host: localhost
    port: 9042
  kafka:
    host: "kafka-headless.kafka.svc.cluster.local"
    port: 9092
  zookeeper:
    host: "zookeeper-headless.kafka.svc.cluster.local"
    port: 2181
  image:
    registry: ""

checkpointing:
  enabled: false
  statebackend: null

# Google Cloud Storage Service Account JSON Path
google_service_account_key_path: ""

log4j_console_properties: |
  # This affects logging for both user code and Flink
  rootLogger.level = INFO
  rootLogger.appenderRef.console.ref = ConsoleAppender
  rootLogger.appenderRef.rolling.ref = RollingFileAppender

  # Uncomment this if you want to _only_ change Flink's logging
  logger.flink.name = org.apache.flink
  logger.flink.level = INFO

  # The following lines keep the log level of common libraries/connectors on
  # log level INFO. The root logger does not override this. You have to manually
  # change the log levels here.
  logger.akka.name = akka
  logger.akka.level = ERROR
  logger.kafka.name= org.apache.kafka
  logger.kafka.level = ERROR
  logger.hadoop.name = org.apache.hadoop
  logger.hadoop.level = ERROR
  logger.zookeeper.name = org.apache.zookeeper
  logger.zookeeper.level = ERROR

  # Log all infos to the console
  appender.console.name = ConsoleAppender
  appender.console.type = CONSOLE
  appender.console.layout.type = PatternLayout
  appender.console.layout.pattern = %d{yyyy-MM-dd HH:mm:ss,SSS} %-5p %-60c %x - %m%n

  # Log all infos in the given rolling file
  appender.rolling.name = RollingFileAppender
  appender.rolling.type = RollingFile
  appender.rolling.append = false
  appender.rolling.fileName = ${sys:log.file}
  appender.rolling.filePattern = ${sys:log.file}.%i
  appender.rolling.layout.type = PatternLayout
  appender.rolling.layout.pattern = %d{yyyy-MM-dd HH:mm:ss,SSS} %-5p %-60c %x - %m%n
  appender.rolling.policies.type = Policies
  appender.rolling.policies.size.type = SizeBasedTriggeringPolicy
  appender.rolling.policies.size.size=10MB
  appender.rolling.strategy.type = DefaultRolloverStrategy
  appender.rolling.strategy.max = 5

  # Suppress the irrelevant (wrong) warnings from the Netty channel handler
  logger.netty.name = org.apache.flink.shaded.akka.org.jboss.netty.channel.DefaultChannelPipeline
  logger.netty.level = OFF

base_config: |
  job {
    env = "{{ .Values.global.env }}"
    enable.distributed.checkpointing = {{ .Values.checkpointing.enabled }}
    statebackend {
      base.url = "{{ .Values.checkpointing.statebackend }}"
    }
  }
  kafka {
    broker-servers = "{{ .Values.global.kafka.host }}:{{ .Values.global.kafka.port }}"
    producer.broker-servers = "{{ .Values.global.kafka.host }}:{{ .Values.global.kafka.port }}"
    consumer.broker-servers = "{{ .Values.global.kafka.host }}:{{ .Values.global.kafka.port }}"
    zookeeper = "{{ .Values.global.zookeeper.host }}:{{ .Values.global.zookeeper.port }}"
    producer {
      max-request-size = 10000024
      batch.size = 98304
      linger.ms = 10
      compression = "snappy"
    }
    output.system.event.topic = ${job.env}".system.events"
    output.failed.topic = ${job.env}".failed"
  }

  redis.connection.timeout = 100
  redis {
    host = "{{ .Values.global.redis_dedup.host }}"
    port = {{ .Values.global.redis_dedup.port }}
  }

  redis-meta {
    host = "{{ .Values.global.redis_denorm.host }}"
    port = {{ .Values.global.redis_denorm.port }}
  }

  task {
    parallelism = 1
    consumer.parallelism = 1
    checkpointing.interval = 120000
    checkpointing.pause.between.seconds = 120000
    restart-strategy.attempts = 3
    restart-strategy.delay = 30000 # in milli-seconds
  }

  postgres {
    host = "{{ .Values.global.postgresql.host }}"
    port = "{{ .Values.global.postgresql.port }}"
    maxConnections = "{{ .Values.postgres.max_connections }}"
    sslMode = "{{ .Values.postgres.sslmode }}"
    user = "{{ .Values.global.postgresql.obsrv.user }}"
    password = "{{ .Values.global.postgresql.obsrv.password }}"
    database = "{{ .Values.global.postgresql.obsrv.name }}"
  }

  lms-cassandra {
    host = "{{ .Values.global.cassandra.host }}"
    port = "{{ .Values.global.cassandra.port }}"
  }


serviceAccount:
  # Specifies whether a service account should be created
  create: false
  # Annotations to add to the service account
  annotations: {}
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name: ""

hadoop_core_site: {}

lakehouse-connector:
  lakehouse-connector: |+
    include file("/data/flink/conf/baseconfig.conf")
    kafka {
      input.topic = ${job.env}".hudi.connector.in"
      output.topic = ${job.env}".hudi.connector.out"
      output.invalid.topic = ${job.env}".failed"
      event.max.size = "1048576" # Max is only 1MB
      groupId = ${job.env}"-hudi-writer-group"
      producer {
        max-request-size = 5242880
      }
    }

    task {
      checkpointing.compressed = true
      checkpointing.interval = 120000
      checkpointing.pause.between.seconds = 120000
      restart-strategy.attempts = 3
      restart-strategy.delay = 30000 # in milli-seconds
      parallelism = 1
      consumer.parallelism = 1
      downstream.operators.parallelism = 1
    }

    hudi {
      hms {
        enabled = true
        uri = "thrift://hudi-hms.hms.svc:9083"
        database {
          name = "hms"
          username = "hms"
          password = "hms123"
        }
      }
      table {
        type = "MERGE_ON_READ"
        base.path = "{{ .Values.global.hudi_metadata_bucket }}"
      }
      write {
        tasks = 1
        task.max.memory = 512
        compaction.max.memory = 512
      }
      metadata.enabled = true
      compaction.enabled = true
      delta.commits = 5
      metadata.delta.commits = 5
      write.tasks = 2
      write.batch.size = 256
      compaction.tasks = 1
      index.type = "BLOOM"
      delta.commits = 2
      delta.seconds = 10
      write.batch.size = 256
      compaction.tasks = 2
      write.lock.provider = "org.apache.hudi.client.transaction.lock.InProcessLockProvider"
      write.concurrency.mode = "optimistic_concurrency_control"
      metadata.index.column.stats.enabled = true
      compression.codec = "snappy"
      {{- if eq .Values.global.cloud_storage_provider "azure" }}
      fs.atomic_creation.support = "wasbs"
      {{- end}}
      {{- if eq .Values.global.cloud_storage_provider "aws" }}
      fs.atomic_creation.support = "s3a"
      {{- end}}
    }
  flink-conf: |+
    jobmanager.memory.flink.size: 1024m
    taskmanager.memory.flink.size: 2048m
    taskmanager.numberOfTaskSlots: 1
    jobManager.numberOfTaskSlots: 1
    parallelism.default: 1
    jobmanager.execution.failover-strategy: region
    taskmanager.memory.network.fraction: 0.1
    taskmanager.memory.managed.fraction: 0.3
    heartbeat.timeout: 8000
    heartbeat.interval: 5000
    state.savepoints.dir: file:///tmp
    jobmanager.rpc.address: lakehouse-connector-jobmanager
    blob.server.port: 6124
    jobmanager.rpc.port: 6123
    taskmanager.rpc.port: 6122
  job_classname: org.sunbird.obsrv.streaming.HudiConnectorStreamTask

nameOverride: "lakehouse-connector"